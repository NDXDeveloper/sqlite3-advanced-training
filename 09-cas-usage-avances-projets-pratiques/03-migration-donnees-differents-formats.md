üîù Retour au [Sommaire](/SOMMAIRE.md)
# 9.3 : Migration de donn√©es entre diff√©rents formats

## Introduction : L'art de la migration de donn√©es

Dans le monde r√©el, les donn√©es n'existent jamais dans un seul format. Vous devrez souvent **importer des donn√©es** depuis Excel, CSV, JSON, ou d'autres bases de donn√©es, et parfois **exporter vos donn√©es SQLite** vers ces m√™mes formats.

Cette section vous apprendra √† ma√Ætriser ces migrations de donn√©es de mani√®re robuste et efficace.

### Pourquoi migrer des donn√©es ?

- **Consolidation** : Rassembler des donn√©es √©parpill√©es
- **Changement d'outil** : Passer d'Excel √† une vraie base de donn√©es
- **Int√©gration** : Connecter diff√©rents syst√®mes
- **Sauvegarde** : Exporter pour archivage ou partage
- **Analyse** : Importer pour traitement avec SQLite

### Types de migrations courantes

```
Sources communes ‚Üí SQLite ‚Üí Destinations fr√©quentes

CSV files     ‚Üò         ‚Üó Excel (.xlsx)
Excel files   ‚Üí SQLite ‚Üê JSON files
JSON/XML      ‚Üó         ‚Üò Autres SGBD
Autres SGBD   ‚Üô         ‚Üñ API REST
```

## Pr√©paration de l'environnement

### Installation des biblioth√®ques n√©cessaires

```bash
# Biblioth√®ques pour diff√©rents formats
pip install pandas openpyxl xlsxwriter lxml requests pymysql psycopg2-binary
```

### Structure de projet pour migrations

```
migration_projet/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ source/           # Donn√©es d'origine
‚îÇ   ‚îú‚îÄ‚îÄ target/           # Donn√©es converties
‚îÇ   ‚îî‚îÄ‚îÄ migration.db      # Base SQLite de travail
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ import_csv.py     # Import depuis CSV
‚îÇ   ‚îú‚îÄ‚îÄ import_excel.py   # Import depuis Excel
‚îÇ   ‚îú‚îÄ‚îÄ import_json.py    # Import depuis JSON
‚îÇ   ‚îú‚îÄ‚îÄ export_data.py    # Export vers diff√©rents formats
‚îÇ   ‚îî‚îÄ‚îÄ migrate_db.py     # Migration entre SGBD
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ connections.ini   # Configuration des connexions
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ migration.log     # Journalisation des op√©rations
```

## Classe de base pour les migrations

### Gestionnaire de migration universel

```python
import sqlite3
import pandas as pd
import json
import logging
from datetime import datetime
from pathlib import Path

class MigrateurDonnees:
    def __init__(self, db_path="data/migration.db"):
        self.db_path = db_path
        self.setup_logging()
        self.init_database()

    def setup_logging(self):
        """Configure le syst√®me de logs"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/migration.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def init_database(self):
        """Initialise la base SQLite"""
        with sqlite3.connect(self.db_path) as conn:
            # Table de suivi des migrations
            conn.execute('''
                CREATE TABLE IF NOT EXISTS migrations_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_file TEXT NOT NULL,
                    target_table TEXT NOT NULL,
                    records_processed INTEGER,
                    records_success INTEGER,
                    records_errors INTEGER,
                    migration_date DATETIME DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'COMPLETED',
                    error_details TEXT
                )
            ''')

    def log_migration(self, source_file, target_table, processed, success, errors, status='COMPLETED', error_details=None):
        """Enregistre les d√©tails d'une migration"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT INTO migrations_log
                (source_file, target_table, records_processed, records_success, records_errors, status, error_details)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (source_file, target_table, processed, success, errors, status, error_details))
```

## Migration depuis CSV

### Import CSV robuste avec validation

```python
def importer_csv(self, chemin_csv, nom_table, options=None):
    """Importe un fichier CSV vers SQLite avec validation compl√®te"""

    # Options par d√©faut
    options = options or {
        'encoding': 'utf-8',
        'delimiter': ',',
        'quotechar': '"',
        'skip_rows': 0,
        'clean_headers': True,
        'validate_data': True
    }

    try:
        self.logger.info(f"üöÄ D√©but import CSV : {chemin_csv} ‚Üí {nom_table}")

        # 1. Lecture et inspection du fichier
        if not Path(chemin_csv).exists():
            raise FileNotFoundError(f"Fichier non trouv√© : {chemin_csv}")

        # D√©tecter l'encodage automatiquement
        encoding = self.detecter_encodage(chemin_csv)

        # Lire le CSV avec pandas
        df = pd.read_csv(
            chemin_csv,
            encoding=encoding,
            delimiter=options['delimiter'],
            quotechar=options['quotechar'],
            skiprows=options['skip_rows'],
            low_memory=False
        )

        self.logger.info(f"üìä {len(df)} lignes lues depuis {Path(chemin_csv).name}")

        # 2. Nettoyage des en-t√™tes
        if options['clean_headers']:
            df = self.nettoyer_entetes(df)

        # 3. Validation et nettoyage des donn√©es
        if options['validate_data']:
            df_clean, erreurs = self.nettoyer_donnees_csv(df)
            self.logger.info(f"‚úÖ {len(df_clean)} lignes valides, {len(erreurs)} erreurs")
        else:
            df_clean = df
            erreurs = []

        # 4. Import dans SQLite
        with sqlite3.connect(self.db_path) as conn:
            # Cr√©er la table avec les bons types
            self.creer_table_depuis_dataframe(conn, nom_table, df_clean)

            # Ins√©rer les donn√©es
            df_clean.to_sql(nom_table, conn, if_exists='replace', index=False)

        # 5. Logging de la migration
        self.log_migration(
            chemin_csv, nom_table,
            len(df), len(df_clean), len(erreurs)
        )

        self.logger.info(f"‚úÖ Import termin√© : {len(df_clean)} lignes dans {nom_table}")

        return {
            'success': True,
            'records_imported': len(df_clean),
            'errors': erreurs,
            'table_name': nom_table
        }

    except Exception as e:
        error_msg = f"Erreur lors de l'import CSV : {str(e)}"
        self.logger.error(error_msg)

        self.log_migration(
            chemin_csv, nom_table,
            0, 0, 1, 'FAILED', error_msg
        )

        return {
            'success': False,
            'error': error_msg
        }

def detecter_encodage(self, chemin_fichier):
    """D√©tecte automatiquement l'encodage d'un fichier"""
    import chardet

    with open(chemin_fichier, 'rb') as f:
        # Lire les premiers 10000 octets pour d√©tecter l'encodage
        raw_data = f.read(10000)
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']

        self.logger.info(f"üîç Encodage d√©tect√© : {encoding} (confiance: {confidence:.2f})")

        # Fallback vers utf-8 si la confiance est faible
        if confidence < 0.7:
            encoding = 'utf-8'
            self.logger.warning("‚ö†Ô∏è Confiance faible, utilisation d'UTF-8 par d√©faut")

        return encoding

def nettoyer_entetes(self, df):
    """Nettoie les en-t√™tes de colonnes"""
    # Supprimer les espaces et caract√®res sp√©ciaux
    df.columns = df.columns.str.strip()
    df.columns = df.columns.str.replace(' ', '_')
    df.columns = df.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True)
    df.columns = df.columns.str.lower()

    # G√©rer les colonnes dupliqu√©es
    df.columns = pd.io.common.dedup_names(df.columns, is_potential_multiindex=False)

    return df

def nettoyer_donnees_csv(self, df):
    """Nettoie et valide les donn√©es CSV"""
    df_clean = df.copy()
    erreurs = []

    # Supprimer les lignes compl√®tement vides
    avant = len(df_clean)
    df_clean = df_clean.dropna(how='all')
    lignes_vides = avant - len(df_clean)

    if lignes_vides > 0:
        self.logger.info(f"üßπ {lignes_vides} lignes vides supprim√©es")

    # Nettoyer les cha√Ænes de caract√®res
    for col in df_clean.select_dtypes(include=['object']).columns:
        df_clean[col] = df_clean[col].astype(str).str.strip()
        df_clean[col] = df_clean[col].replace('nan', None)

    # Convertir les types de donn√©es quand c'est possible
    for col in df_clean.columns:
        df_clean, col_erreurs = self.convertir_type_colonne(df_clean, col)
        erreurs.extend(col_erreurs)

    return df_clean, erreurs

def convertir_type_colonne(self, df, nom_colonne):
    """Tente de convertir une colonne vers le type le plus appropri√©"""
    erreurs = []

    try:
        # Tenter conversion num√©rique
        if df[nom_colonne].dtype == 'object':
            # Essayer d'abord les entiers
            try:
                df[nom_colonne] = pd.to_numeric(df[nom_colonne], errors='coerce').astype('Int64')
            except:
                # Sinon essayer les d√©cimaux
                try:
                    df[nom_colonne] = pd.to_numeric(df[nom_colonne], errors='coerce')
                except:
                    # Essayer les dates
                    try:
                        df[nom_colonne] = pd.to_datetime(df[nom_colonne], errors='coerce')
                    except:
                        pass  # Garder comme texte
    except Exception as e:
        erreurs.append(f"Erreur conversion colonne {nom_colonne}: {str(e)}")

    return df, erreurs
```

## Migration depuis Excel

### Import Excel multi-feuilles

```python
def importer_excel(self, chemin_excel, options=None):
    """Importe un fichier Excel (toutes les feuilles ou sp√©cifiques)"""

    options = options or {
        'sheets': None,  # None = toutes les feuilles
        'header_row': 0,
        'skip_rows': None,
        'clean_data': True
    }

    try:
        self.logger.info(f"üìó Import Excel : {chemin_excel}")

        # Lire le fichier Excel
        excel_file = pd.ExcelFile(chemin_excel)
        self.logger.info(f"üìã Feuilles disponibles : {excel_file.sheet_names}")

        # D√©terminer quelles feuilles importer
        if options['sheets'] is None:
            sheets_to_import = excel_file.sheet_names
        else:
            sheets_to_import = options['sheets']

        resultats = {}

        for sheet_name in sheets_to_import:
            self.logger.info(f"üìÑ Traitement de la feuille : {sheet_name}")

            try:
                # Lire la feuille
                df = pd.read_excel(
                    chemin_excel,
                    sheet_name=sheet_name,
                    header=options['header_row'],
                    skiprows=options['skip_rows']
                )

                # Nettoyer le nom de table (nom de feuille)
                nom_table = self.nettoyer_nom_table(sheet_name)

                # Nettoyage des donn√©es si demand√©
                if options['clean_data']:
                    df = self.nettoyer_entetes(df)
                    df, erreurs = self.nettoyer_donnees_csv(df)

                # Import dans SQLite
                with sqlite3.connect(self.db_path) as conn:
                    df.to_sql(nom_table, conn, if_exists='replace', index=False)

                self.log_migration(
                    f"{chemin_excel}:{sheet_name}", nom_table,
                    len(df), len(df), 0
                )

                resultats[sheet_name] = {
                    'table_name': nom_table,
                    'records_imported': len(df),
                    'success': True
                }

                self.logger.info(f"‚úÖ Feuille {sheet_name} ‚Üí table {nom_table} ({len(df)} lignes)")

            except Exception as e:
                error_msg = f"Erreur feuille {sheet_name}: {str(e)}"
                self.logger.error(error_msg)
                resultats[sheet_name] = {'success': False, 'error': error_msg}

        return resultats

    except Exception as e:
        error_msg = f"Erreur lors de l'import Excel : {str(e)}"
        self.logger.error(error_msg)
        return {'success': False, 'error': error_msg}

def nettoyer_nom_table(self, nom):
    """Nettoie un nom pour en faire un nom de table SQLite valide"""
    import re

    # Supprimer caract√®res sp√©ciaux et espaces
    nom_clean = re.sub(r'[^a-zA-Z0-9_]', '_', nom)
    nom_clean = nom_clean.strip('_').lower()

    # S'assurer que √ßa commence par une lettre
    if nom_clean and nom_clean[0].isdigit():
        nom_clean = 'table_' + nom_clean

    # Limiter la longueur
    if len(nom_clean) > 30:
        nom_clean = nom_clean[:30]

    return nom_clean or 'table_import'
```

## Migration depuis/vers JSON

### Import JSON avec structures complexes

```python
def importer_json(self, chemin_json, nom_table, options=None):
    """Importe des donn√©es JSON vers SQLite"""

    options = options or {
        'normalize': True,     # Aplatir les objets imbriqu√©s
        'max_level': 2,        # Niveau max d'imbrication
        'array_handling': 'expand'  # 'expand' ou 'serialize'
    }

    try:
        self.logger.info(f"üìÑ Import JSON : {chemin_json}")

        # Lire le fichier JSON
        with open(chemin_json, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # D√©terminer la structure
        if isinstance(data, dict):
            # Si c'est un objet, chercher les tableaux
            df = self.extraire_dataframe_depuis_objet(data, options)
        elif isinstance(data, list):
            # Si c'est un tableau
            df = self.extraire_dataframe_depuis_tableau(data, options)
        else:
            raise ValueError("Format JSON non support√©")

        self.logger.info(f"üìä {len(df)} enregistrements extraits")

        # Import dans SQLite
        with sqlite3.connect(self.db_path) as conn:
            df.to_sql(nom_table, conn, if_exists='replace', index=False)

        self.log_migration(
            chemin_json, nom_table,
            len(df), len(df), 0
        )

        self.logger.info(f"‚úÖ Import JSON termin√© : {len(df)} lignes dans {nom_table}")

        return {
            'success': True,
            'records_imported': len(df),
            'table_name': nom_table
        }

    except Exception as e:
        error_msg = f"Erreur lors de l'import JSON : {str(e)}"
        self.logger.error(error_msg)
        return {'success': False, 'error': error_msg}

def extraire_dataframe_depuis_tableau(self, data, options):
    """Extrait un DataFrame depuis un tableau JSON"""

    if options['normalize']:
        # Utiliser json_normalize pour aplatir
        df = pd.json_normalize(
            data,
            max_level=options['max_level'],
            errors='ignore'
        )
    else:
        # Cr√©ation directe du DataFrame
        df = pd.DataFrame(data)

    return df

def extraire_dataframe_depuis_objet(self, data, options):
    """Extrait un DataFrame depuis un objet JSON complexe"""

    # Chercher le premier tableau dans l'objet
    for key, value in data.items():
        if isinstance(value, list) and len(value) > 0:
            self.logger.info(f"üìã Utilisation du tableau '{key}' ({len(value)} √©l√©ments)")
            return self.extraire_dataframe_depuis_tableau(value, options)

    # Si pas de tableau trouv√©, cr√©er un DataFrame avec l'objet
    return pd.DataFrame([data])
```

### Export vers JSON

```python
def exporter_vers_json(self, nom_table, chemin_sortie, options=None):
    """Exporte une table SQLite vers JSON"""

    options = options or {
        'format': 'records',    # 'records', 'index', 'values'
        'indent': 2,
        'date_format': 'iso',
        'include_metadata': True
    }

    try:
        self.logger.info(f"üì§ Export {nom_table} ‚Üí {chemin_sortie}")

        # R√©cup√©rer les donn√©es
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(f"SELECT * FROM {nom_table}", conn)

        # Pr√©parer les donn√©es pour JSON
        if options['date_format'] == 'iso':
            # Convertir les dates au format ISO
            for col in df.select_dtypes(include=['datetime64']).columns:
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')

        # Structure de sortie
        output_data = {
            'data': df.to_dict(orient=options['format']),
            'metadata': {
                'table_name': nom_table,
                'record_count': len(df),
                'columns': list(df.columns),
                'export_date': datetime.now().isoformat(),
                'source_database': self.db_path
            } if options['include_metadata'] else None
        }

        # Si pas de m√©tadonn√©es, exporter directement les donn√©es
        if not options['include_metadata']:
            output_data = output_data['data']

        # √âcrire le fichier JSON
        with open(chemin_sortie, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=options['indent'], ensure_ascii=False, default=str)

        self.logger.info(f"‚úÖ Export JSON termin√© : {len(df)} enregistrements")

        return {
            'success': True,
            'records_exported': len(df),
            'output_file': chemin_sortie
        }

    except Exception as e:
        error_msg = f"Erreur lors de l'export JSON : {str(e)}"
        self.logger.error(error_msg)
        return {'success': False, 'error': error_msg}
```

## Migration vers Excel

### Export Excel multi-feuilles avec formatage

```python
def exporter_vers_excel(self, tables, chemin_sortie, options=None):
    """Exporte plusieurs tables vers un fichier Excel multi-feuilles"""

    options = options or {
        'format_headers': True,
        'auto_adjust_columns': True,
        'add_summary': True,
        'freeze_panes': True
    }

    try:
        self.logger.info(f"üìä Export Excel : {len(tables)} tables ‚Üí {chemin_sortie}")

        with pd.ExcelWriter(chemin_sortie, engine='xlsxwriter') as writer:

            # Donn√©es de r√©sum√©
            summary_data = []

            for nom_table in tables:
                self.logger.info(f"üìã Export table : {nom_table}")

                # R√©cup√©rer les donn√©es
                with sqlite3.connect(self.db_path) as conn:
                    df = pd.read_sql_query(f"SELECT * FROM {nom_table}", conn)

                # Nom de feuille (limit√© √† 31 caract√®res pour Excel)
                sheet_name = nom_table[:31]

                # √âcrire dans Excel
                df.to_excel(writer, sheet_name=sheet_name, index=False)

                # Formatage de la feuille
                if options['format_headers'] or options['auto_adjust_columns']:
                    worksheet = writer.sheets[sheet_name]
                    workbook = writer.book

                    # Format des en-t√™tes
                    if options['format_headers']:
                        header_format = workbook.add_format({
                            'bold': True,
                            'text_wrap': True,
                            'valign': 'top',
                            'fg_color': '#D7E4BC',
                            'border': 1
                        })

                        for col_num, value in enumerate(df.columns.values):
                            worksheet.write(0, col_num, value, header_format)

                    # Ajustement automatique des colonnes
                    if options['auto_adjust_columns']:
                        for i, col in enumerate(df.columns):
                            # Calculer la largeur max
                            max_length = max(
                                df[col].astype(str).map(len).max(),
                                len(str(col))
                            )
                            worksheet.set_column(i, i, min(max_length + 2, 50))

                    # Figer les volets
                    if options['freeze_panes']:
                        worksheet.freeze_panes(1, 0)

                # Ajouter aux stats de r√©sum√©
                summary_data.append({
                    'Table': nom_table,
                    'Feuille': sheet_name,
                    'Lignes': len(df),
                    'Colonnes': len(df.columns),
                    'Taille_MB': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
                })

            # Cr√©er une feuille de r√©sum√©
            if options['add_summary']:
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='_R√©sum√©', index=False)

                # Formatage du r√©sum√©
                summary_sheet = writer.sheets['_R√©sum√©']
                summary_format = writer.book.add_format({
                    'bold': True,
                    'fg_color': '#4F81BD',
                    'font_color': 'white'
                })

                for col_num, value in enumerate(summary_df.columns.values):
                    summary_sheet.write(0, col_num, value, summary_format)

        total_records = sum(item['Lignes'] for item in summary_data)
        self.logger.info(f"‚úÖ Export Excel termin√© : {total_records} enregistrements total")

        return {
            'success': True,
            'tables_exported': len(tables),
            'total_records': total_records,
            'output_file': chemin_sortie
        }

    except Exception as e:
        error_msg = f"Erreur lors de l'export Excel : {str(e)}"
        self.logger.error(error_msg)
        return {'success': False, 'error': error_msg}
```

## Migration entre SGBD

### Migration vers MySQL/PostgreSQL

```python
def migrer_vers_sgbd(self, tables, sgbd_config, options=None):
    """Migre des tables SQLite vers un autre SGBD"""

    options = options or {
        'batch_size': 1000,
        'create_tables': True,
        'truncate_target': False,
        'handle_conflicts': 'replace'  # 'replace', 'ignore', 'error'
    }

    try:
        # Connexion au SGBD cible
        if sgbd_config['type'] == 'mysql':
            import pymysql
            connection = pymysql.connect(**sgbd_config['params'])
        elif sgbd_config['type'] == 'postgresql':
            import psycopg2
            connection = psycopg2.connect(**sgbd_config['params'])
        else:
            raise ValueError(f"SGBD non support√© : {sgbd_config['type']}")

        self.logger.info(f"üîÑ Migration vers {sgbd_config['type']}")

        resultats = {}

        for nom_table in tables:
            self.logger.info(f"üìã Migration table : {nom_table}")

            try:
                # R√©cup√©rer les donn√©es SQLite
                with sqlite3.connect(self.db_path) as sqlite_conn:
                    df = pd.read_sql_query(f"SELECT * FROM {nom_table}", sqlite_conn)

                # Cr√©er la table dans le SGBD cible si n√©cessaire
                if options['create_tables']:
                    self.creer_table_sgbd(connection, nom_table, df, sgbd_config['type'])

                # Vider la table si demand√©
                if options['truncate_target']:
                    with connection.cursor() as cursor:
                        cursor.execute(f"TRUNCATE TABLE {nom_table}")
                    connection.commit()

                # Ins√©rer par lots
                records_inserted = self.inserer_par_lots(
                    connection, nom_table, df,
                    options['batch_size'], sgbd_config['type']
                )

                resultats[nom_table] = {
                    'success': True,
                    'records_migrated': records_inserted
                }

                self.logger.info(f"‚úÖ Table {nom_table} migr√©e : {records_inserted} lignes")

            except Exception as e:
                error_msg = f"Erreur table {nom_table}: {str(e)}"
                self.logger.error(error_msg)
                resultats[nom_table] = {'success': False, 'error': error_msg}

        connection.close()
        return resultats

    except Exception as e:
        error_msg = f"Erreur migration SGBD : {str(e)}"
        self.logger.error(error_msg)
        return {'success': False, 'error': error_msg}

def creer_table_sgbd(self, connection, nom_table, df, sgbd_type):
    """Cr√©e une table dans le SGBD cible bas√©e sur le DataFrame"""

    # Mapping des types SQLite vers autres SGBD
    type_mapping = {
        'mysql': {
            'object': 'TEXT',
            'int64': 'BIGINT',
            'float64': 'DOUBLE',
            'datetime64[ns]': 'DATETIME',
            'bool': 'BOOLEAN'
        },
        'postgresql': {
            'object': 'TEXT',
            'int64': 'BIGINT',
            'float64': 'DOUBLE PRECISION',
            'datetime64[ns]': 'TIMESTAMP',
            'bool': 'BOOLEAN'
        }
    }

    # Construire la requ√™te CREATE TABLE
    columns = []
    for col_name, dtype in df.dtypes.items():
        col_type = type_mapping[sgbd_type].get(str(dtype), 'TEXT')
        columns.append(f"{col_name} {col_type}")

    create_sql = f"CREATE TABLE IF NOT EXISTS {nom_table} ({', '.join(columns)})"

    with connection.cursor() as cursor:
        cursor.execute(create_sql)
    connection.commit()
```

## Exercices pratiques

### Exercice 1 : Migration compl√®te d'un syst√®me

Vous avez re√ßu des donn√©es clients dans diff√©rents formats :

```
data/
‚îú‚îÄ‚îÄ clients_2023.csv      # Donn√©es clients ann√©e derni√®re
‚îú‚îÄ‚îÄ commandes.xlsx        # Feuilles : commandes, produits, livraisons
‚îî‚îÄ‚îÄ feedback.json         # Avis clients au format JSON
```

**Objectif :** Cr√©er une base SQLite unifi√©e avec toutes ces donn√©es.

**√âtapes :**
1. Analyser chaque fichier pour comprendre sa structure
2. Nettoyer et valider les donn√©es
3. Cr√©er un sch√©ma relationnel coh√©rent
4. Importer toutes les donn√©es
5. Cr√©er des vues pour faciliter les requ√™tes

### Exercice 2 : Synchronisation bidirectionnelle

Cr√©ez un syst√®me qui :

1. **Importe** des donn√©es depuis une API REST
2. **Les traite** dans SQLite
3. **Exporte** les r√©sultats vers Excel
4. **Synchronise** les modifications

```python
# Exemple de structure
def synchroniser_donnees():
    # 1. R√©cup√©rer depuis API
    donnees_api = requests.get('https://api.exemple.com/data').json()

    # 2. Importer dans SQLite
    migrator.importer_json_data(donnees_api, 'donnees_externes')

    # 3. Traitement avec SQL
    # ... requ√™tes de traitement ...

    # 4. Export vers Excel
    migrator.exporter_vers_excel(['donnees_traitees'], 'rapport_final.xlsx')

    # 5. Renvoyer les modifications √† l'API
    # ... code de synchronisation ...
```

### Exercice 3 : Migration de legacy vers moderne

Migrez un ancien syst√®me Access (.mdb) vers SQLite moderne :

**D√©fis √† r√©soudre :**
- Encodage des caract√®res sp√©ciaux
- Types de donn√©es obsol√®tes
- Relations complexes
- Donn√©es corrompues ou incoh√©rentes

## Automatisation des migrations

### Script de migration automatis√©

```python
class AutoMigrator:
    def __init__(self, config_file="config/migration_config.json"):
        with open(config_file, 'r') as f:
            self.config = json.load(f)

        self.migrator = MigrateurDonnees()

    def executer_plan_migration(self):
        """Ex√©cute un plan de migration complet"""

        plan = self.config['migration_plan']

        for etape in plan:
            self.logger.info(f"üéØ √âtape : {etape['name']}")

            if etape['type'] == 'import_csv':
                self.migrator.importer_csv(
                    etape['source'],
                    etape['target_table'],
                    etape.get('options', {})
                )

            elif etape['type'] == 'import_excel':
                self.migrator.importer_excel(
                    etape['source'],
                    etape.get('options', {})
                )

            elif etape['type'] == 'import_json':
                self.migrator.importer_json(
                    etape['source'],
                    etape['target_table'],
                    etape.get('options', {})
                )

            elif etape['type'] == 'transform':
                self.executer_transformations(etape['transformations'])

            elif etape['type'] == 'export':
                self.executer_export(etape)

            elif etape['type'] == 'validate':
                self.valider_donnees(etape['validations'])

    def executer_transformations(self, transformations):
        """Ex√©cute des transformations SQL"""
        with sqlite3.connect(self.migrator.db_path) as conn:
            for transform in transformations:
                self.logger.info(f"üîÑ Transformation : {transform['name']}")
                conn.execute(transform['sql'])
                conn.commit()

    def valider_donnees(self, validations):
        """Valide la qualit√© des donn√©es migr√©es"""
        with sqlite3.connect(self.migrator.db_path) as conn:
            for validation in validations:
                cursor = conn.execute(validation['sql'])
                result = cursor.fetchone()[0]

                if validation['operator'] == 'equals' and result != validation['expected']:
                    raise ValueError(f"Validation √©chou√©e : {validation['name']}")
                elif validation['operator'] == 'greater_than' and result <= validation['expected']:
                    raise ValueError(f"Validation √©chou√©e : {validation['name']}")

                self.logger.info(f"‚úÖ Validation r√©ussie : {validation['name']}")

# Exemple de fichier de configuration migration_config.json
migration_config_example = {
    "migration_plan": [
        {
            "name": "Import clients CSV",
            "type": "import_csv",
            "source": "data/source/clients.csv",
            "target_table": "clients",
            "options": {
                "encoding": "utf-8",
                "clean_headers": True,
                "validate_data": True
            }
        },
        {
            "name": "Import commandes Excel",
            "type": "import_excel",
            "source": "data/source/commandes.xlsx",
            "options": {
                "sheets": ["commandes", "produits"],
                "clean_data": True
            }
        },
        {
            "name": "Normalisation des donn√©es",
            "type": "transform",
            "transformations": [
                {
                    "name": "Nettoyage emails",
                    "sql": "UPDATE clients SET email = LOWER(TRIM(email)) WHERE email IS NOT NULL"
                },
                {
                    "name": "Cr√©ation table commandes_enrichies",
                    "sql": """
                        CREATE TABLE commandes_enrichies AS
                        SELECT
                            c.*,
                            cl.nom as nom_client,
                            cl.email as email_client
                        FROM commandes c
                        JOIN clients cl ON c.client_id = cl.id
                    """
                }
            ]
        },
        {
            "name": "Validation des donn√©es",
            "type": "validate",
            "validations": [
                {
                    "name": "Nombre de clients import√©s",
                    "sql": "SELECT COUNT(*) FROM clients",
                    "operator": "greater_than",
                    "expected": 0
                },
                {
                    "name": "Emails uniques",
                    "sql": "SELECT COUNT(*) FROM clients WHERE email IS NOT NULL GROUP BY email HAVING COUNT(*) > 1",
                    "operator": "equals",
                    "expected": 0
                }
            ]
        },
        {
            "name": "Export final",
            "type": "export",
            "format": "excel",
            "tables": ["clients", "commandes_enrichies"],
            "output": "data/target/donnees_migrees.xlsx"
        }
    ]
}
```

### Monitoring et alertes

```python
class MigrationMonitor:
    def __init__(self, migrator):
        self.migrator = migrator
        self.seuils = {
            'taux_erreur_max': 0.05,  # 5% d'erreurs max
            'temps_execution_max': 3600,  # 1 heure max
            'taille_min_donnees': 100  # Minimum 100 enregistrements
        }

    def surveiller_migration(self, fonction_migration, *args, **kwargs):
        """Surveille une migration et g√©n√®re des alertes"""

        debut = time.time()

        try:
            # Ex√©cuter la migration
            resultat = fonction_migration(*args, **kwargs)

            fin = time.time()
            duree = fin - debut

            # V√©rifications
            self.verifier_performances(duree, resultat)
            self.verifier_qualite_donnees(resultat)

            # Log de succ√®s
            self.logger.info(f"‚úÖ Migration surveill√©e termin√©e en {duree:.2f}s")

            return resultat

        except Exception as e:
            # Alerte en cas d'√©chec
            self.envoyer_alerte_echec(str(e))
            raise

    def verifier_performances(self, duree, resultat):
        """V√©rifie les performances de la migration"""

        if duree > self.seuils['temps_execution_max']:
            self.envoyer_alerte_performance(f"Migration trop lente: {duree:.2f}s")

        if 'records_imported' in resultat:
            if resultat['records_imported'] < self.seuils['taille_min_donnees']:
                self.envoyer_alerte_donnees(
                    f"Trop peu de donn√©es import√©es: {resultat['records_imported']}"
                )

    def verifier_qualite_donnees(self, resultat):
        """V√©rifie la qualit√© des donn√©es migr√©es"""

        if 'errors' in resultat and len(resultat['errors']) > 0:
            taux_erreur = len(resultat['errors']) / resultat.get('records_processed', 1)

            if taux_erreur > self.seuils['taux_erreur_max']:
                self.envoyer_alerte_qualite(
                    f"Taux d'erreur √©lev√©: {taux_erreur:.2%}"
                )

    def envoyer_alerte_echec(self, message):
        """Envoie une alerte en cas d'√©chec de migration"""
        # Ici vous pouvez impl√©menter l'envoi d'email, Slack, etc.
        self.logger.error(f"üö® ALERTE √âCHEC MIGRATION: {message}")

    def envoyer_alerte_performance(self, message):
        """Alerte de performance"""
        self.logger.warning(f"‚ö†Ô∏è ALERTE PERFORMANCE: {message}")

    def envoyer_alerte_donnees(self, message):
        """Alerte sur la quantit√© de donn√©es"""
        self.logger.warning(f"‚ö†Ô∏è ALERTE DONN√âES: {message}")

    def envoyer_alerte_qualite(self, message):
        """Alerte sur la qualit√© des donn√©es"""
        self.logger.warning(f"‚ö†Ô∏è ALERTE QUALIT√â: {message}")
```

## Optimisations pour grandes migrations

### Migration par chunks pour gros volumes

```python
def migrer_gros_volume(self, source_query, target_table, chunk_size=10000):
    """Migre de gros volumes par petits lots"""

    offset = 0
    total_migre = 0

    while True:
        # R√©cup√©rer un chunk
        chunk_query = f"{source_query} LIMIT {chunk_size} OFFSET {offset}"

        with sqlite3.connect(self.db_path) as conn:
            df_chunk = pd.read_sql_query(chunk_query, conn)

        # Arr√™ter si plus de donn√©es
        if len(df_chunk) == 0:
            break

        # Traiter le chunk
        self.traiter_chunk(df_chunk, target_table)

        total_migre += len(df_chunk)
        offset += chunk_size

        self.logger.info(f"üì¶ Chunk trait√©: {total_migre} enregistrements")

        # Pause pour √©viter la surcharge
        time.sleep(0.1)

    self.logger.info(f"‚úÖ Migration gros volume termin√©e: {total_migre} enregistrements")

def traiter_chunk(self, df_chunk, target_table):
    """Traite un chunk de donn√©es"""

    # Nettoyage du chunk
    df_clean = self.nettoyer_donnees_csv(df_chunk)[0]

    # Insert dans la table cible
    with sqlite3.connect(self.db_path) as conn:
        df_clean.to_sql(target_table, conn, if_exists='append', index=False)
```

### Parall√©lisation des migrations

```python
import concurrent.futures
from multiprocessing import cpu_count

def migrer_en_parallele(self, liste_fichiers, max_workers=None):
    """Migre plusieurs fichiers en parall√®le"""

    if max_workers is None:
        max_workers = min(cpu_count(), len(liste_fichiers))

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:

        # Soumettre toutes les t√¢ches
        futures = {}
        for fichier_info in liste_fichiers:
            future = executor.submit(
                self.migrer_fichier_unique,
                fichier_info['path'],
                fichier_info['table'],
                fichier_info.get('options', {})
            )
            futures[future] = fichier_info['path']

        # Collecter les r√©sultats
        resultats = {}
        for future in concurrent.futures.as_completed(futures):
            fichier = futures[future]
            try:
                resultat = future.result()
                resultats[fichier] = resultat
                self.logger.info(f"‚úÖ Migration parall√®le termin√©e: {fichier}")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur migration parall√®le {fichier}: {e}")
                resultats[fichier] = {'success': False, 'error': str(e)}

    return resultats

def migrer_fichier_unique(self, chemin_fichier, nom_table, options):
    """Migre un seul fichier (utilis√© pour la parall√©lisation)"""

    extension = Path(chemin_fichier).suffix.lower()

    if extension == '.csv':
        return self.importer_csv(chemin_fichier, nom_table, options)
    elif extension in ['.xlsx', '.xls']:
        return self.importer_excel(chemin_fichier, options)
    elif extension == '.json':
        return self.importer_json(chemin_fichier, nom_table, options)
    else:
        raise ValueError(f"Format de fichier non support√©: {extension}")
```

## Bonnes pratiques pour la migration

### ‚úÖ √Ä faire

- **Sauvegarder** les donn√©es originales avant migration
- **Valider** les donn√©es apr√®s chaque √©tape
- **Logger** toutes les op√©rations pour audit
- **Tester** sur un √©chantillon avant migration compl√®te
- **Documenter** le processus de migration
- **Pr√©voir** un plan de rollback en cas d'√©chec
- **Monitorer** les performances et la qualit√©

### ‚ùå √Ä √©viter

- **Migrer sans validation** pr√©alable des donn√©es
- **Ignorer les erreurs** de conversion de types
- **N√©gliger l'encodage** des caract√®res
- **Oublier les contraintes** de cl√©s √©trang√®res
- **Surcharger** la m√©moire avec de gros datasets
- **Migrer en production** sans tests pr√©alables

## Cas d'usage avanc√©s

### Migration incr√©mentale

```python
def migration_incrementale(self, source_table, target_table, date_column):
    """Migre seulement les nouvelles donn√©es depuis la derni√®re migration"""

    # R√©cup√©rer la date de derni√®re migration
    with sqlite3.connect(self.db_path) as conn:
        cursor = conn.execute(
            "SELECT MAX(migration_date) FROM migrations_log WHERE target_table = ?",
            (target_table,)
        )
        derniere_migration = cursor.fetchone()[0]

    # Construire la requ√™te incr√©mentale
    if derniere_migration:
        where_clause = f"WHERE {date_column} > '{derniere_migration}'"
    else:
        where_clause = ""

    query = f"SELECT * FROM {source_table} {where_clause}"

    # Ex√©cuter la migration incr√©mentale
    return self.migrer_avec_requete(query, target_table)
```

### Synchronisation bidirectionnelle

```python
def synchroniser_bidirectionnelle(self, table_locale, table_distante, cle_primaire):
    """Synchronise les modifications dans les deux sens"""

    # 1. Identifier les conflits
    conflits = self.detecter_conflits(table_locale, table_distante, cle_primaire)

    # 2. R√©soudre les conflits (strat√©gie √† d√©finir)
    self.resoudre_conflits(conflits)

    # 3. Appliquer les modifications locales vers distant
    self.pousser_modifications_locales(table_locale, table_distante)

    # 4. R√©cup√©rer les modifications distantes
    self.tirer_modifications_distantes(table_distante, table_locale)

    # 5. Marquer la synchronisation
    self.marquer_synchronisation(table_locale)
```

## Outils de debugging et diagnostic

### Analyseur de qualit√© de donn√©es

```python
def analyser_qualite_migration(self, nom_table):
    """Analyse la qualit√© des donn√©es apr√®s migration"""

    with sqlite3.connect(self.db_path) as conn:
        # Statistiques g√©n√©rales
        stats = {
            'total_rows': conn.execute(f"SELECT COUNT(*) FROM {nom_table}").fetchone()[0],
            'columns': []
        }

        # Analyse par colonne
        cursor = conn.execute(f"PRAGMA table_info({nom_table})")
        colonnes = cursor.fetchall()

        for colonne in colonnes:
            nom_col = colonne[1]

            # Stats de la colonne
            col_stats = {
                'name': nom_col,
                'type': colonne[2],
                'null_count': conn.execute(f"SELECT COUNT(*) FROM {nom_table} WHERE {nom_col} IS NULL").fetchone()[0],
                'unique_count': conn.execute(f"SELECT COUNT(DISTINCT {nom_col}) FROM {nom_table}").fetchone()[0],
                'sample_values': []
            }

            # √âchantillon de valeurs
            cursor = conn.execute(f"SELECT DISTINCT {nom_col} FROM {nom_table} LIMIT 5")
            col_stats['sample_values'] = [row[0] for row in cursor.fetchall()]

            stats['columns'].append(col_stats)

    return stats

def generer_rapport_qualite(self, stats):
    """G√©n√®re un rapport de qualit√© des donn√©es"""

    print("=" * 60)
    print(f"RAPPORT DE QUALIT√â - {stats['total_rows']} enregistrements")
    print("=" * 60)

    for col in stats['columns']:
        null_pct = (col['null_count'] / stats['total_rows']) * 100
        unique_pct = (col['unique_count'] / stats['total_rows']) * 100

        print(f"\nüìä {col['name']} ({col['type']})")
        print(f"   Valeurs nulles: {col['null_count']} ({null_pct:.1f}%)")
        print(f"   Valeurs uniques: {col['unique_count']} ({unique_pct:.1f}%)")
        print(f"   √âchantillon: {col['sample_values']}")

        # Alertes qualit√©
        if null_pct > 50:
            print(f"   ‚ö†Ô∏è ATTENTION: Trop de valeurs nulles")
        if unique_pct < 5 and col['name'] != 'id':
            print(f"   ‚ö†Ô∏è ATTENTION: Peu de diversit√© dans les donn√©es")
```

## Conclusion

La migration de donn√©es est un aspect crucial de tout projet impliquant SQLite. Une approche m√©thodique, avec validation, logging et monitoring, garantit des migrations fiables et tra√ßables.

Les scripts et techniques pr√©sent√©s dans cette section constituent une bo√Æte √† outils compl√®te pour g√©rer tous vos besoins de migration de donn√©es, du simple import CSV √† la synchronisation complexe entre syst√®mes.

---

**Points cl√©s √† retenir :**

- **Planifiez** toujours vos migrations avec des tests pr√©alables
- **Validez** les donn√©es √† chaque √©tape du processus
- **Automatisez** les t√¢ches r√©p√©titives avec des scripts robustes
- **Surveillez** la qualit√© et les performances des migrations
- **Documentez** vos processus pour faciliter la maintenance
- **Pr√©parez** des strat√©gies de rollback en cas de probl√®me
- **Testez** sur des √©chantillons avant les migrations compl√®tes

‚è≠Ô∏è
